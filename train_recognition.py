import os
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from PoseRecognitionDataset import PoseRecognitionDataset
from PoseRecognitionNetwork import PoseRecognitionNetwork
from torch.utils.tensorboard import SummaryWriter
import argparse
import torch.nn.functional as F

def hard_triplet_mining(embeddings, labels, margin=1.0):
    """ 
    Hard triplet mining strategy: Select the hardest positive and hardest negative for each anchor.
    embeddings: The embeddings generated by the model.
    labels: Corresponding labels for the embeddings.
    margin: The margin for triplet loss.
    """
    triplet_loss_fn = torch.nn.TripletMarginLoss(margin=margin)
    
    triplet_loss = 0.0
    batch_size = embeddings.size(0)
    
    for i in range(batch_size):
        anchor = embeddings[i]
        positive_mask = labels == labels[i]
        negative_mask = labels != labels[i]
        
        # Select the hardest positive (max distance)
        positive_distances = F.pairwise_distance(anchor.unsqueeze(0), embeddings[positive_mask])
        hardest_positive_idx = torch.argmax(positive_distances).item()
        hardest_positive = embeddings[positive_mask][hardest_positive_idx]

        # Select the hardest negative (min distance)
        negative_distances = F.pairwise_distance(anchor.unsqueeze(0), embeddings[negative_mask])
        hardest_negative_idx = torch.argmin(negative_distances).item()
        hardest_negative = embeddings[negative_mask][hardest_negative_idx]

        # Calculate triplet loss for this triplet
        triplet_loss += triplet_loss_fn(anchor, hardest_positive, hardest_negative)
    
    return triplet_loss / batch_size


def train_and_evaluate(data_dir, num_epochs=50, embedding_size=10, pretrained_lstm_path=None, save_path='runs/train/weights', tensorboard_path='runs/train/tensorboard'):
    # Ensure the save_path directory exists
    os.makedirs(save_path, exist_ok=True)

    # Load train, validation, and test datasets
    train_dataset = PoseRecognitionDataset(data_dir, sequence_length=30, split='train')
    val_dataset = PoseRecognitionDataset(data_dir, sequence_length=30, split='validation') if "validation" in os.listdir(os.path.join(data_dir, os.listdir(data_dir)[0])) else None
    test_dataset = PoseRecognitionDataset(data_dir, sequence_length=30, split='test') if "test" in os.listdir(os.path.join(data_dir, os.listdir(data_dir)[0])) else None
    #     val_dataset = PoseRecognitionDataset(data_dir, sequence_length=30, split='validation')
    # if "test" in os.listdir(os.path.join(data_dir, os.listdir(data_dir)[0])):
    #     test_dataset = PoseRecognitionDataset(data_dir, sequence_length=30, split='test')

    # If train dataset is not found, we cannot proceed
    if not train_dataset:
        raise ValueError("Training dataset is required but not found.")

    # Create DataLoaders for available datasets
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)

    if val_dataset:
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)
    else:
        val_loader = None

    if test_dataset:
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)
    else:
        test_loader = None

    # Save the class labels to a text file
    classes_file = os.path.join(save_path, 'classes.txt')
    with open(classes_file, 'w') as f:
        for class_name in train_dataset.class_labels:
            f.write(f"{class_name}\n")
    print(f"Class labels saved to {classes_file}")

    # Model and Optimizer
    input_size = 34  # Based on your dataset (17 keypoints * 2 dimensions)
    hidden_size = 128
    num_layers = 2

    model = PoseRecognitionNetwork(input_size, hidden_size, num_layers, embedding_size, pretrained_lstm_path=pretrained_lstm_path)
    optimizer = optim.Adam(model.fc.parameters(), lr=0.001)

    # Set up TensorBoard
    writer = SummaryWriter(tensorboard_path)

    best_val_loss = float('inf')
    best_model_path = os.path.join(save_path, 'best.pt')
    last_model_path = os.path.join(save_path, 'last.pt')

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        for i, (inputs, labels) in enumerate(train_loader):
            optimizer.zero_grad()
            embeddings = model(inputs)

            # Perform hard triplet mining and compute triplet loss
            loss = hard_triplet_mining(embeddings, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 10:.4f}')
            writer.add_scalar('Training Loss', running_loss / 10, epoch * len(train_loader) + i)
            running_loss = 0.0

        # Validate the model if validation data is provided
        if val_loader:
            model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for inputs, labels in val_loader:
                    embeddings = model(inputs)
                    val_loss += hard_triplet_mining(embeddings, labels).item()

            val_loss /= len(val_loader)
            print(f'Epoch {epoch + 1}, Validation Loss: {val_loss:.4f}')
            writer.add_scalar('Validation Loss', val_loss, epoch)

            # Save best model based on validation loss
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(model, best_model_path)
                print(f'Best model saved with validation loss: {best_val_loss:.4f}')

        # Save the last model
        torch.save(model, last_model_path)
        if not val_loader:
            torch.save(model, best_model_path)

    print('Training finished.')

    # Test the model if test data is provided
    if test_loader:
        model = torch.load(best_model_path)
        model.eval()
        test_loss = 0.0
        with torch.no_grad():
            for inputs, labels in test_loader:
                embeddings = model(inputs)
                test_loss += hard_triplet_mining(embeddings, labels).item()

        test_loss /= len(test_loader)
        print(f'Test Loss: {test_loss:.4f}')
        writer.add_scalar('Test Loss', test_loss)

    writer.close()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Train and evaluate PoseRecognitionNetwork with Triplet Loss.')

    parser.add_argument('--data_dir', type=str, required=True, help='Path to the main data directory containing train/validation/test subdirectories')
    parser.add_argument('--num_epochs', type=int, default=250, help='Number of epochs to train')
    parser.add_argument('--embedding_size', type=int, default=64, help='Size of the embedding vector')
    parser.add_argument('--pretrained_lstm_path', type=str, default=None, help='Path to the pretrained LSTM weights')
    parser.add_argument('--save_path', type=str, default='runs_recognition/train/weights', help='Directory to save models')
    parser.add_argument('--tensorboard_path', type=str, default='runs_recognition/train/tensorboard', help='Directory to save TensorBoard logs')

    args = parser.parse_args()

    train_and_evaluate(args.data_dir, args.num_epochs, args.embedding_size, args.pretrained_lstm_path, args.save_path, args.tensorboard_path)

# python3 train_recognition.py --data_dir person_recognition_dataset --pretrained_lstm_path runs/train/weights/best.pt --save_path runs_recognition/train2/weights --tensorboard_path runs_recognition/train2/tensorboard