import os
import subprocess
import json
import torch
from PoseActionClassifier import PoseActionClassifierLSTM
from PoseActionDataset import PoseActionDataset
import argparse
import numpy as np
import glob

# def run_alphapose(video_path, alphapose_dir, output_dir, cfg_file, checkpoint_file, detector):
def run_alphapose(video_path, alphapose_dir, output_dir, cfg_file, checkpoint_file):
    # Change the working directory to AlphaPose directory
    original_cwd = os.getcwd()
    os.chdir(alphapose_dir)

    # Define the output directory for AlphaPose results
    os.makedirs(output_dir, exist_ok=True)
    
    # Run AlphaPose
    command = [
        'python', 'scripts/demo_inference.py',  # Since we're now in the AlphaPose directory
        '--cfg', cfg_file,
        '--checkpoint', checkpoint_file,
        '--video', video_path,
        '--outdir', output_dir,
        # '--detector', detector,
        '--save_video'
    ]
    
    try:
        subprocess.run(command, check=True)
    finally:
        # Revert back to the original working directory
        os.chdir(original_cwd)
    
    # The JSON file with keypoints results
    result_json = os.path.join(output_dir, 'alphapose-results.json')
    
    if not os.path.exists(result_json):
        raise FileNotFoundError(f"Expected JSON result file not found: {result_json}")
    
    return result_json

def load_model(model_path, num_classes):
    model = PoseActionClassifierLSTM(num_classes)
    model.load_state_dict(torch.load(model_path))
    model.eval()
    return model

def predict_action(model, result_json, sequence_length):
    # Load the AlphaPose results
    with open(result_json, 'r') as f:
        people_list = json.load(f)

    # Prepare keypoints sequence using the dataset method
    dataset = PoseActionDataset(sequence_length=sequence_length)  # No poses_dir needed for inference
    keypoints_seq = dataset._get_main_person_keypoints(people_list, result_json)
    keypoints_seq = dataset._adjust_sequence_length(np.array(keypoints_seq))

    # Run prediction
    inputs = torch.tensor(keypoints_seq, dtype=torch.float32).unsqueeze(0)  # Add batch dimension
    outputs = model(inputs)
    _, predicted = torch.max(outputs, 1)

    return predicted.item()

# def infer_action(video_path, alphapose_dir, output_dir, model_path, cfg_file, checkpoint_file, detector):
def infer_action(video_path, alphapose_dir, output_dir, model_path, cfg_file, checkpoint_file):
    # Run AlphaPose on the video and get the result JSON
    result_json = run_alphapose(video_path, alphapose_dir, output_dir, cfg_file, checkpoint_file)
    
    classes_file = os.path.join(os.path.dirname(model_path), 'classes.txt')
    with open(classes_file, 'r') as f:
        class_names = [line.strip() for line in f.readlines()]

    # Load the trained model
    num_classes = len(class_names)
    model = load_model(model_path, num_classes)
    
    # Predict the action class
    action_class_index = predict_action(model, result_json, sequence_length=30)  # Adjust sequence_length to your training
    action_class_name = class_names[action_class_index]
    
    # Find the video file generated by AlphaPose
    original_video_name = os.path.splitext(os.path.basename(video_path))[0]
    video_files = glob.glob(os.path.join(output_dir, f"AlphaPose_{original_video_name}.*"))
    if len(video_files) == 0:
        raise FileNotFoundError(f"Generated video file for {original_video_name} not found in {output_dir}")
    elif len(video_files) > 1:
        raise RuntimeError(f"Multiple generated video files found for {original_video_name} in {output_dir}: {video_files}")
    
    alphapose_video_path = video_files[0]
    
    # Save the video with keypoints with the action class in the filename
    output_video_path = os.path.join(output_dir, f"{original_video_name}_{action_class_name}{os.path.splitext(alphapose_video_path)[1]}")
    os.rename(alphapose_video_path, output_video_path)
    
    print(f"Action predicted: {action_class_name}")
    print(f"Video with keypoints saved to: {output_video_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Run action recognition on a video using a trained model.')

    parser.add_argument('--video_path', type=str, required=True, help='Path to the video file')
    parser.add_argument('--alphapose_dir', type=str, default='/home/aniyazi/masters_degree/AlphaPose/', help='Path to the AlphaPose directory')
    parser.add_argument('--output_dir', type=str, required=True, default="/infer_result", help='Directory to save results')
    parser.add_argument('--model_path', type=str, required=True, help='Path to the trained action recognition model')
    parser.add_argument('--cfg_file', type=str, default='/home/aniyazi/masters_degree/AlphaPose/configs/coco/resnet/256x192_res50_lr1e-3_1x.yaml', help='Path to the AlphaPose config file')
    parser.add_argument('--checkpoint_file', type=str, default='/home/aniyazi/masters_degree/AlphaPose/pretrained_models/fast_res50_256x192.pth', help='Path to the AlphaPose checkpoint file')
    # parser.add_argument('--class_names', type=str, nargs='+', default=['deadlifting', 'exercising arm', 'front raises', 'jogging', 'lunge', 'punching bag', 'push up', 'situp'], help='List of class names for the action recognition model')
    # parser.add_argument('--detector', type=str, default='/home/aniyazi/masters_degree/AlphaPose/detector/yolo/', help='Detector type to be used in AlphaPose (e.g., yolo, fastreid)')

    args = parser.parse_args()

    # infer_action(args.video_path, args.alphapose_dir, args.output_dir, args.model_path, args.cfg_file, args.checkpoint_file, args.detector)
    infer_action(args.video_path, args.alphapose_dir, args.output_dir, args.model_path, args.cfg_file, args.checkpoint_file)

#sample run command
    
# python infer.py --video_path /home/aniyazi/masters_degree/action_recognition/sample_videos/4.mp4 --output_dir /home/aniyazi/masters_degree/action_recognition/samples_output/4 --model_path runs/train/weights/best.pt